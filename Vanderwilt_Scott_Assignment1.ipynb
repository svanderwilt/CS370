{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 1.5146 - accuracy: 0.5988 - val_loss: 0.7707 - val_accuracy: 0.8318\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.6037 - accuracy: 0.8496 - val_loss: 0.4542 - val_accuracy: 0.8828\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.4360 - accuracy: 0.8816 - val_loss: 0.3708 - val_accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.3747 - accuracy: 0.8955 - val_loss: 0.3326 - val_accuracy: 0.9078\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.3411 - accuracy: 0.9030 - val_loss: 0.3077 - val_accuracy: 0.9131\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.3180 - accuracy: 0.9093 - val_loss: 0.2902 - val_accuracy: 0.9163\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.3002 - accuracy: 0.9139 - val_loss: 0.2759 - val_accuracy: 0.9216\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2855 - accuracy: 0.9187 - val_loss: 0.2636 - val_accuracy: 0.9253\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2734 - accuracy: 0.9211 - val_loss: 0.2540 - val_accuracy: 0.9278\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2625 - accuracy: 0.9248 - val_loss: 0.2459 - val_accuracy: 0.9294\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.2528 - accuracy: 0.9277 - val_loss: 0.2368 - val_accuracy: 0.9325\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2436 - accuracy: 0.9298 - val_loss: 0.2305 - val_accuracy: 0.9348\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.2356 - accuracy: 0.9322 - val_loss: 0.2259 - val_accuracy: 0.9354\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.2284 - accuracy: 0.9346 - val_loss: 0.2180 - val_accuracy: 0.9383\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2213 - accuracy: 0.9371 - val_loss: 0.2115 - val_accuracy: 0.9389\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2148 - accuracy: 0.9387 - val_loss: 0.2061 - val_accuracy: 0.9413\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.2087 - accuracy: 0.9401 - val_loss: 0.2009 - val_accuracy: 0.9422\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 8s 158us/step - loss: 0.2028 - accuracy: 0.9412 - val_loss: 0.1963 - val_accuracy: 0.9438\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1972 - accuracy: 0.9427 - val_loss: 0.1920 - val_accuracy: 0.9463\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.1922 - accuracy: 0.9447 - val_loss: 0.1876 - val_accuracy: 0.9467\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score:  0.19026985982358455\n",
      "Test accuracy:  0.9434999823570251\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "#10 outputs\n",
    "#final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE,\n",
    "                   validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score: \", score[0])\n",
    "print('Test accuracy: ', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 1.3884 - accuracy: 0.6589 - val_loss: 0.8937 - val_accuracy: 0.8256\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.7959 - accuracy: 0.8266 - val_loss: 0.6564 - val_accuracy: 0.8571\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.6451 - accuracy: 0.8480 - val_loss: 0.5609 - val_accuracy: 0.8693\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.5722 - accuracy: 0.8597 - val_loss: 0.5080 - val_accuracy: 0.8794\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.86 - 1s 22us/step - loss: 0.5278 - accuracy: 0.8673 - val_loss: 0.4739 - val_accuracy: 0.8844\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4973 - accuracy: 0.8730 - val_loss: 0.4496 - val_accuracy: 0.8892\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4748 - accuracy: 0.8767 - val_loss: 0.4316 - val_accuracy: 0.8920\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.4573 - accuracy: 0.8801 - val_loss: 0.4171 - val_accuracy: 0.8947\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.4432 - accuracy: 0.8831 - val_loss: 0.4057 - val_accuracy: 0.8957\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4316 - accuracy: 0.8852 - val_loss: 0.3960 - val_accuracy: 0.8973\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.4217 - accuracy: 0.8872 - val_loss: 0.3880 - val_accuracy: 0.8997\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.4132 - accuracy: 0.8885 - val_loss: 0.3810 - val_accuracy: 0.9005\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.4059 - accuracy: 0.8899 - val_loss: 0.3749 - val_accuracy: 0.9018\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3993 - accuracy: 0.8913 - val_loss: 0.3695 - val_accuracy: 0.9022\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3935 - accuracy: 0.8926 - val_loss: 0.3648 - val_accuracy: 0.9038\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3883 - accuracy: 0.8937 - val_loss: 0.3605 - val_accuracy: 0.9043\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3836 - accuracy: 0.8948 - val_loss: 0.3567 - val_accuracy: 0.9051\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3793 - accuracy: 0.8960 - val_loss: 0.3531 - val_accuracy: 0.9062\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3754 - accuracy: 0.8971 - val_loss: 0.3498 - val_accuracy: 0.9068\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3718 - accuracy: 0.8974 - val_loss: 0.3469 - val_accuracy: 0.9078\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Test score:  0.34841032955646517\n",
      "Test accuracy:  0.9064000248908997\n"
     ]
    }
   ],
   "source": [
    "# Same training but with no hidden layers\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Removed hidden layers\n",
    "#10 outputs\n",
    "#final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE,\n",
    "                   validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score: \", score[0])\n",
    "print('Test accuracy: ', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 23:23:33.743054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 2s 3ms/step - loss: 1.6010 - accuracy: 0.5893 - val_loss: 0.7604 - val_accuracy: 0.8276\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5713 - accuracy: 0.8483 - val_loss: 0.4268 - val_accuracy: 0.8842\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4083 - accuracy: 0.8852 - val_loss: 0.3532 - val_accuracy: 0.9003\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3514 - accuracy: 0.8994 - val_loss: 0.3163 - val_accuracy: 0.9070\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3176 - accuracy: 0.9081 - val_loss: 0.2887 - val_accuracy: 0.9153\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2933 - accuracy: 0.9157 - val_loss: 0.2692 - val_accuracy: 0.9202\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.9210 - val_loss: 0.2532 - val_accuracy: 0.9263\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2584 - accuracy: 0.9255 - val_loss: 0.2431 - val_accuracy: 0.9288\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2446 - accuracy: 0.9289 - val_loss: 0.2313 - val_accuracy: 0.9336\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2323 - accuracy: 0.9325 - val_loss: 0.2196 - val_accuracy: 0.9365\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2214 - accuracy: 0.9359 - val_loss: 0.2170 - val_accuracy: 0.9377\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2112 - accuracy: 0.9388 - val_loss: 0.2054 - val_accuracy: 0.9413\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2023 - accuracy: 0.9413 - val_loss: 0.1969 - val_accuracy: 0.9445\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1939 - accuracy: 0.9438 - val_loss: 0.1907 - val_accuracy: 0.9459\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1865 - accuracy: 0.9456 - val_loss: 0.1869 - val_accuracy: 0.9459\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1790 - accuracy: 0.9481 - val_loss: 0.1838 - val_accuracy: 0.9482\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1726 - accuracy: 0.9499 - val_loss: 0.1736 - val_accuracy: 0.9513\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1664 - accuracy: 0.9517 - val_loss: 0.1698 - val_accuracy: 0.9506\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1606 - accuracy: 0.9535 - val_loss: 0.1680 - val_accuracy: 0.9513\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1549 - accuracy: 0.9553 - val_loss: 0.1657 - val_accuracy: 0.9523\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9483\n",
      "Test score:  0.16103623807430267\n",
      "Test accuracy:  0.9483000040054321\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "#10 outputs\n",
    "#final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE,\n",
    "                   validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score: \", score[0])\n",
    "print('Test accuracy: ', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "    In the first training there are two hidden layers.  This resulted in a 94.675% accuracy on the training set, 94.34% accuracy on the validation, and 94.3% accuracy on the test set\n",
    "    in only 20 trainings.\n",
    "    In the second training, I removed the hidden layers.  This decreased the accuracy to 89.74% on the training set, 90.64% accuracy on the validation and 90.6% accuracy on the test set\n",
    "    with all other parameters equal.\n",
    "    While the speed of the second training was slightly better than the first, the accuracy was greatly improved upon with the addition of the hidden layers.\n",
    "    In the third training, I put in three hidden layers.  This improved our accuracy to 95.59% on the training set, 95.44% on the validation, and 95.4% on the test set.  \n",
    "    These percentages are significantly higher than the second training and about 1% higher than the first accross the board.\n",
    "    There is a definite tradeoff between accuracy and training time with the addition of hidden layers.  The first two tests show a training accuracy below the test accuracy, which indicates\n",
    "    that I didn't train long enough.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
